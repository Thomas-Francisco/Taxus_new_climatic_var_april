---
title: "GF_corrected_outliers_detection_new_var"
author: "Thomas Francisco"
date: "2024-04-30"
output: html_document
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(cache = FALSE, warning = FALSE)
#download gradientforest package
#install.packages("gradientForest", repos=c("http://R-Forge.R-project.org",
#"http://cran.at.r-project.org"),dependencies=TRUE)
library(gradientForest)
library(dplyr)
library(tidyr)
library(writexl)
library(VennDiagram)
library(radiant.data) #for row_names_to_columns
library(textshape) #for colnames_to_row.names
```

In this script, we will perform an GF analysis corrected for population structure. To see some explanation on GF, see script GF-raw_outliers_detection

GF doesn't have an option to correct for population structure. To perform GF by correcting for population structure, we need to use as genomic dataset already corrected for population structure. We cannot use the scale population structure matrix of BAYPASS because we need a dataframe and not a matrix but we can used the genotypic dataset from LFMM that is corrected for population structure using latent factor.


**All the steps are not explained here, see GF_raw_outliers_detection script for more info and GF_outliers_detection step_by_step**

    1. We load the data: 
    
Climatic data 
```{r climatic_data}
#climatic data
Past_climatic <- read.csv("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/Climatic_data/new_selection/Past_new_6_Climatic_data_scale_df.csv",sep=";",dec=",")
vars <- colnames(Past_climatic[,-c(1:2)])
```



We load the corrected LFMM geno dataframe. The genomic data is then the matrix from LFMM so individual level for 475 indiv , 8616 SNPs imputed and with MAF correction
```{r load genotypic dataset corrected for populations structure}
load("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/genetic_new/GEA/Genomic_matrix_corrected_from_LFMM_T_adapcon_gentree.Rdata")
corrected_geno_data <- Genomic_matrix_corrected_from_LFMM_T_adapcon_gentree
#we need to transform the individual dataframe into a population-level dataframe

#add ind into a column
corrected_geno_data_ind <- rownames_to_column(corrected_geno_data, "VCF_ID")
```

We need to pass the genomic data at the individual level to the population level: 
```{r geno indiv-level to population-level}

#add the population info
#meta_data
meta_data_pop <- read.csv("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/Populations/taxus_sample_29pop.csv",h=T,sep=";",dec=",")

meta_data_vcf <- read.csv("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/Samples/samples_taxus_baccata_adapcon_gentree.csv",h=T,sep=";",dec=",")

geno_pop_info <- merge(meta_data_vcf,corrected_geno_data_ind, "VCF_ID" )


#formatting the genomic data
data_frequencies_num <- geno_pop_info[,-c(1:3)] %>% #keeping only the snps
  apply(2,as.numeric) /2 #we divided by 2 because of the format of genomic data: 0,1,2 and for allelic frequencies and we want 0,0.5, 1


#dataset with all information and genomic data in the right format
data_frequencies_num_tot <- data.frame(geno_pop_info[,c(1:3)],data_frequencies_num)

#calculation of allelic frequencies
allelic_frequencies <-data_frequencies_num_tot %>% dplyr::select(-c("VCF_ID","Country","Population")) %>% #remove non genomic data from the dataset
  group_by(data_frequencies_num_tot$Population) %>% #we want the allelic frequencies at the population level so we grouped
  summarise_at(vars(everything()),funs(mean),na.rm=T) %>% #calculate the mean for each snp per pop
    ungroup() %>%
as.data.frame()


#Pop with row.names
allelic_frequencies_f <- allelic_frequencies %>% column_to_rownames('data_frequencies_num_tot$Population')


```


    2/3. Perform the GF candidates detection
    
We used the function from the script GF_RAW_outliers_detection. 
```{r function to perform the GF and output the results in 1 code}

Run_GF_and_select_outliers <- function(genomic_matrix, climatic_data, ntree, cores,nbr_loci_distrib,vars,x,path){

  #GF function
  runGF <- function(alFreq,  envTab, vars, ntree, 
                  cores, indLoci){

  require(data.table)
  require(gradientForest)
  require(parallel)
  require(foreach)
  require(doParallel)
  library(doParallel)
  library(foreach)
  library(parallel)
  library(gradientForest)
  library(data.table)
  
  if(identical(envTab$Population,rownames(alFreq))==F){
    stop("Populations are not in the same order in the genomic and climatic tables.")
  }
  
  # create custom object to hold output 
  gfOutObj <- setClass("gfOutObj", slots = c(alFreq="data.frame", imp="list"))

  # run in parallel if fitting SNPs individually
  if(indLoci==T & !is.na(cores)){
    # fit gf model to each SNP individually
    cl <- makeCluster(cores, setup_strategy = "sequential")
    registerDoParallel(cl)
    
    gfMods <- foreach(k=1:ncol(alFreq), .verbose=F, .packages=c("gradientForest"), .errorhandling = c("pass")) %dopar%{
      locus <- data.frame(alFreq[,k])
      names(locus) <- colnames(alFreq)[k]
      gf.mod <- gradientForest(data.frame(envTab[, vars], locus), 
                               predictor.vars=vars, response.vars=colnames(alFreq)[k], 
                               corr.threshold=0.5, ntree=ntree, trace=T)
    if(!is.null(gf.mod)){
        imps <- importance(gf.mod)
        imps <- imps[order(names(imps))]
        data.frame(imps, SNP = colnames(alFreq)[k])
      }
    }
    
    stopCluster(cl)
    return(gfOutObj(alFreq = data.frame(alFreq), imp = gfMods))
  } else {
    # run all SNPs at once if not fitting individually
    gf.mod <- gradientForest(data.frame(envTab[, vars], alFreq), 
                             predictor.vars=vars, response.vars=colnames(alFreq), 
                             corr.threshold=0.5, ntree=ntree, trace=T)
    
    return(gfOutObj(alFreq = data.frame(alFreq), imp = gfMods))
  }
}
  #run GF
  GF_test <- runGF(genomic_matrix,climatic_data,vars,ntree=ntree, 
                  cores=cores, indLoci=T)
  
  #extract the loci correlated to the climate
  Extract_correlation_loci_climate<- GF_test@imp
loci_correlated_climate <- Filter(function(x) !inherits(x, "error"),  Extract_correlation_loci_climate)

#extracting R^2 values
gfR2tab <- function(gfMods.list){
  gfMods.list <- gfMods.list
  i=1
  while(is.null(gfMods.list[[i]])){i=i+1}
  tab <- do.call(rbind, gfMods.list)
  vrNm <- rep(row.names(tab)[1:nrow(gfMods.list[[i]])], 
              nrow(tab)/nrow(gfMods.list[[i]]))
  tab <- data.frame(variable=vrNm, tab)
  tab <- reshape2::dcast(tab, SNP~variable, value.var="imps")
  totalR2 <- rowSums(tab[,-1])
  return(data.frame(tab, totalR2=totalR2))}


dataset_R2_loci_climate <- gfR2tab(loci_correlated_climate)

  #select randomly the SNPs, we selected 20% of all SNPs to create the null distribution

name_neutral_snps <- sample(dataset_R2_loci_climate$SNP,nbr_loci_distrib,replace = F)

neutral_snps_set <- dataset_R2_loci_climate %>% 
    filter(SNP %in% name_neutral_snps)

#hist neutral 
 neutral_R2_distrib<-hist(neutral_snps_set$totalR2)
 
 #name
neutral_R2_distrib<-hist(neutral_snps_set$totalR2)


#save the histogram
 png(filename=paste0(path,"neutral_R2_distrib",x,".png"))

# a histogram we want to save
hist(neutral_snps_set$totalR2)

# call this function to save the file 
dev.off()
 
#empirical pvalues
empirical_pvalues <- sapply(1:nrow(dataset_R2_loci_climate), function(x, dataset_R2_loci_climate, name_neutral_snps, neutral_snps_set){
    snps2Rank <- rbind(dataset_R2_loci_climate[x,], neutral_snps_set) %>% 
      distinct() %>% 
      dplyr::select(-SNP)
    P <- apply(snps2Rank, 2, function(y){
      rankSNP <- frank(y)
      return(1-rankSNP[1]/length(rankSNP))
    })}, dataset_R2_loci_climate, neutral_snps, neutral_snps_set)
  

  # format output as data.frame
  empirical_pvalues_df <- t(empirical_pvalues)
  colnames(empirical_pvalues_df) <- paste("pval_", colnames(empirical_pvalues_df), sep="")
  empirical_pvalues_df <- data.frame(dataset_R2_loci_climate, empirical_pvalues_df)

    #visualise the pvalues distribution
  pvalues_distribution <- hist(empirical_pvalues_df$pval_totalR2)
  
  
  #save the histogram
png(filename=paste0(path,"pvalues_distribution",x,".png"))

# a histogram we want to save
hist(empirical_pvalues_df$pval_totalR2)

# call this function to save the file 
dev.off()
  # Return the pvalues 
  return(empirical_pvalues_df)

}

vars <- colnames(Past_climatic[,-c(1:2)])
path <- "C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/GF/corrected_lfmm/"

#for(x in 1:5){
#  name_file <- paste0("Run",x)
#  Run <- Run_GF_and_select_outliers(allelic_frequencies_f, Past_climatic, 500, 4,600,vars,x=x,path)
#  applied(name_file, Run)
#}
```

We save the runs of GF_corrected for consistency in downstream analysis by not reruns it each times and getting different results (because it's a machine learning approach)

```{r save }
#for(x in 1:5){
#  save(paste0("Run",x),file=paste0("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/GEA_new_var/GF_corrected/Run",x,".Rdata"))
#}
```



    4. Selecting a threshold to identify candidates
   
   
We can laod the run to skip the steps 1,2 and 3. 
```{r load RUNs}
for(x in 1:5){
  load(paste0("C:/Users/tfrancisco/Documents/Thèse/Data/Espèces/Taxus_baccata/GEA_new_var/GF_corrected/Run",x,".Rdata"))
}
```
    

Now we want to identified candidates. To do that, we can calculate 2 types of thresholds:  
**rank pvalues threshold:**  
        - rank based 1%  
        - rank based 0.5%  
        
**pvalues threshold:**  
        - pvalues 0.05  
        - pvalues 0.01  
        
```{r calculation of thresholds}
for(i in 1:5){
  data_name <- paste0("Run",i)
  
  data <- get(data_name)
  
   #top 1%
 outliers_top1perc_GF <- data[,c(1,15)] %>% 
  arrange(pval_totalR2) %>%
slice(1:(0.01*nrow(data))) %>%  #slice(1:(0.01*nrow(.)))
  as.data.frame()
 
 assign(paste0("Run",i,"_top1SNP"),outliers_top1perc_GF)
 
   #top 0.5%
 outliers_top0.5perc_GF <- data[,c(1,15)] %>% 
  arrange(pval_totalR2) %>%
slice(1:(0.005*nrow(data))) %>%  #slice(1:(0.01*nrow(.)))
  as.data.frame()
 
 assign(paste0("Run",i,"_top0.5SNP"),outliers_top0.5perc_GF)
 
 #pvalues < 0.05
outliers_pv05 <- data[,c(1,15)] %>% filter(pval_totalR2<0.05) %>% pull(SNP) 

 assign(paste0("Run",i,"_outliers_pv0.05"),outliers_pv05)

#pvalues < 0.01
outliers_pv0.01 <- data[,c(1,15)] %>% filter(pval_totalR2<0.01) %>% pull(SNP)
 
 assign(paste0("Run",i,"_outliers_pv0.01"),outliers_pv0.01)


}
```


One important step is to compare the results of the different runs for each threshold.
We realized venn_diagram plots to visualize the number of common candidates across runs
```{r venn_diagram representation for comparison across runs}
  #candidates 0.01
  venn.diagram(x = list(Run1_outliers_pv0.01, Run2_outliers_pv0.01, Run3_outliers_pv0.01, Run4_outliers_pv0.01, Run5_outliers_pv0.01),
        category.names = c("RUN1" , "RUN2 ","RUN3","RUN4","RUN5"),
        filename = "C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/GF/corrected_lfmm/venn_diagramm_GF_raw_pv0.01.png",fill = c("#45B6AA", B = "#D45176", C = "#91A6CE", D = "#86AD4C","#33A5CE"),
alpha = 0.30,
print.mode=c("raw","percent"),
imagetype="png",
output=TRUE)


  #candidates 0.05
venn.diagram(x = list(Run1_outliers_pv0.05, Run2_outliers_pv0.05, Run3_outliers_pv0.05, Run4_outliers_pv0.05, Run5_outliers_pv0.05),
        category.names = c("RUN1" , "RUN2 ","RUN3","RUN4","RUN5"),
        filename = "C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/GF/corrected_lfmm/venn_diagramm_GF_raw_pv0.05.png",fill = c("#45B6AA", B = "#D45176", C = "#91A6CE", D = "#86AD4C","#33A5CE"),
alpha = 0.30,
print.mode=c("raw","percent"),
imagetype="png",
output=TRUE
)


 #top 1%
venn.diagram(x = list(Run1_top1SNP[,1], Run2_top1SNP[,1], Run3_top1SNP[,1], Run4_top1SNP[,1], Run5_top1SNP[,1]),
        category.names = c("RUN1" , "RUN2 ","RUN3","RUN4","RUN5"),
        filename = "C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/GF/corrected_lfmm/venn_diagramm_GF_raw_top1.png",fill = c("#45B6AA", B = "#D45176", C = "#91A6CE", D = "#86AD4C","#33A5CE"),
alpha = 0.30,
print.mode=c("raw","percent"),
imagetype="png",
output=TRUE
)

 #top 0.5%
venn.diagram(x = list(Run1_top0.5SNP[,1], Run2_top0.5SNP[,1], Run3_top0.5SNP[,1],Run4_top0.5SNP[,1], Run5_top0.5SNP[,1]),
        category.names = c("RUN1" , "RUN2 ","RUN3","RUN4","RUN5"),
        filename = "C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/GF/corrected_lfmm/venn_diagramm_GF_raw_top0.5.png",fill = c("#45B6AA", B = "#D45176", C = "#91A6CE", D = "#86AD4C","#33A5CE"),
alpha = 0.30,
print.mode=c("raw","percent"),
imagetype="png",
output=TRUE
)
```

Globally, we can see that only a very small part of all the SNPs identified as candidates are common across runs. This result inform us that using GF as candidates detection methods corrected for population structure is maybe not the best method. 
We will not use the candidates identified across runs by GF-corrected

```{r select the overlapping candidates across runs and save them}

#Select only the candidates identified in all 5 runs
outliers_rank_based_top1perc_GF_corrected <- Reduce(intersect, list(Run1_top1SNP[,1], Run2_top1SNP[,1], Run3_top1SNP[,1], Run4_top1SNP[,1], Run5_top1SNP[,1]))

#save
save(outliers_rank_based_top1perc_GF_corrected,file="C:/Users/tfrancisco/Documents/Thèse/Results/species/taxus/GEA_new_var/outliers/outliers_rank_based_top1perc_GF_corrected.Rdata", force=T)
```
